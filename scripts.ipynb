{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tulime network analysis\n",
    "\n",
    "The data regarding the project was first obtained through an Gsheet compiled partially by an organization component and partially by me, employing the data provided by the association thrugh mails and Whatsapp conversations. <br>\n",
    "This sheet served, once completed, to extract data regarding single projects and convert them into CSVs in order to later access them through python and perform analysis and operation on data through <a href=\"https://pandas.pydata.org/\">Pandas</a> library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_in_dir(dir):\n",
    "    for filename in os.listdir(dir):\n",
    "        f = os.path.join(dir, filename)\n",
    "        if os.path.isfile(f) and f[-4:] == \".csv\":\n",
    "            yield f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all the random integers for the beneficiaris of Maji in Pomerini, Kitowo and Mwawambala were extracted.<br>\n",
    "A corresponding number of rows was added to the excel file with \"Name*n*\" in the \"NAME\" field and the village name in the corresponding one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_kitowo=random.randint(2500, 3000)\n",
    "rand_pomerini=random.randint(850, 900)\n",
    "rand_mwawambala=random.randint(850, 900)\n",
    "\n",
    "df_maji = pd.DataFrame(index=np.arange(rand_kitowo+rand_pomerini+rand_mwawambala), columns=[\"NAME\", \"VILLAGE\", \"AGE\", \"SEX\", \"FAMILY\", \"START\"])\n",
    "idx = df_maji.index.values.astype(int)\n",
    "idx = idx [1:]\n",
    "for el in idx:\n",
    "    df_maji.loc[el, \"NAME\"] = \"Name\"+str(el)\n",
    "    if el in range(1, rand_kitowo):\n",
    "        df_maji.loc[el, \"VILLAGE\"] = \"Kitowo\"\n",
    "    elif el in range(rand_kitowo, rand_kitowo+rand_pomerini):\n",
    "        df_maji.loc[el, \"VILLAGE\"] = \"Pomerini\"\n",
    "    elif el in range(rand_kitowo+rand_pomerini, rand_kitowo+rand_pomerini+rand_mwawambala):\n",
    "        df_maji.loc[el, \"VILLAGE\"] = \"Mawambala\"\n",
    "df_maji=df_maji.dropna(thresh=2)\n",
    "#df_maji.to_csv(r\"projects_csv/Maji.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first step we need to obtain the distribution of values absent in other CSVs from the that present a more complete amount of information. To do so we first of all extract the data we have and organize it in dictionaries (to conver in CSVs) containing the values of interest and the numbre of peope in realtion to each of the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_scan= [file for file in get_all_in_dir(\"projects_csv\")]\n",
    "\n",
    "sex_dict = dict()\n",
    "age_dict=dict()\n",
    "age_dict_children = dict()\n",
    "family_dict=dict()\n",
    "\n",
    "for file in to_scan:\n",
    "    df = pd.read_csv(file, encoding=\"utf-8\")\n",
    "    df=df.fillna(\"NaN\")\n",
    "    for col in df.columns:\n",
    "        if col == \"SEX\":\n",
    "            for el in df[col]:\n",
    "                if el in sex_dict.keys():\n",
    "                    sex_dict[el]+=1\n",
    "                elif el !=\"NaN\":\n",
    "                    sex_dict[el]=1\n",
    "\n",
    "        if col == \"AGE\" and file not in [\"projects_csv\\TupoPamoja.csv\", \"projects_csv\\PiantalaSubito.csv\"]:\n",
    "            for el in df[col]:\n",
    "                if el in age_dict.keys():\n",
    "                    age_dict[el]+=1\n",
    "                elif el !=\"NaN\":\n",
    "                    age_dict[el]=1\n",
    "\n",
    "        if col== \"AGE\":\n",
    "            for el in df[col]:\n",
    "                if el in age_dict_children.keys():\n",
    "                    age_dict_children[el]+=1\n",
    "                elif el !=\"NaN\":\n",
    "                    age_dict_children[el]=1\n",
    "\n",
    "        \n",
    "        if col == \"FAMILY\":\n",
    "            for el in df[col]:\n",
    "                if el in family_dict.keys():\n",
    "                    family_dict[el]+=1\n",
    "                elif el !=\"NaN\":\n",
    "                    family_dict[el]=1\n",
    "\n",
    "df_sex = pd.DataFrame.from_dict(sex_dict, orient='index')\n",
    "#df_sex.to_csv(r\"projects_csv/measures/sex_count.csv\")\n",
    "\n",
    "age_dict = dict(sorted(age_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "df_age = pd.DataFrame.from_dict(age_dict, orient='index')\n",
    "#df_age.to_csv(r\"projects_csv/measures/age_count.csv\")\n",
    "\n",
    "age_dict_children = dict(sorted(age_dict_children.items(), key=lambda item: item[1], reverse=True))\n",
    "df_age_children = pd.DataFrame.from_dict(age_dict_children, orient='index')\n",
    "#df_age_children.to_csv(r\"projects_csv/measures/agetotal_count.csv\")\n",
    "\n",
    "family_dict = dict(sorted(family_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "df_family = pd.DataFrame.from_dict(family_dict, orient='index')\n",
    "#df_family.to_csv(r\"projects_csv/measures/family_count.csv\")\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns were then renaimed as \"*VARIABLE*\" and \"PEOPLE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting distributions from the values obtained\n",
    "In order to fill missing gaps in the other CSVs we need to transform the data already in our possess into distributions of variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_sex = 0\n",
    "sum_age = 0\n",
    "sum_agechildren = 0\n",
    "sum_family = 0\n",
    "\n",
    "for el in sex_dict.keys():\n",
    "    sum_sex += sex_dict[el]\n",
    "for el in sex_dict.keys():\n",
    "    sex_dict[el]=sex_dict[el]/sum_sex\n",
    "\n",
    "for el in age_dict.keys():\n",
    "    sum_age += age_dict[el]\n",
    "for el in age_dict.keys():\n",
    "    age_dict[el]=age_dict[el]/sum_age\n",
    "\n",
    "for el in age_dict_children.keys():\n",
    "    sum_agechildren += age_dict_children[el]\n",
    "for el in age_dict_children.keys():\n",
    "    age_dict_children[el]=age_dict_children[el]/sum_agechildren\n",
    "\n",
    "for el in family_dict.keys():\n",
    "    sum_family += family_dict[el]\n",
    "for el in family_dict.keys():\n",
    "    family_dict[el]=family_dict[el]/sum_family\n",
    "\n",
    "df_sex = pd.DataFrame.from_dict(sex_dict, orient='index')\n",
    "#df_sex.to_csv(r\"projects_csv/measures/sex_distribution.csv\")\n",
    "\n",
    "age_dict = dict(sorted(age_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "df_age = pd.DataFrame.from_dict(age_dict, orient='index')\n",
    "#df_age.to_csv(r\"projects_csv/measures/age_distribution.csv\")\n",
    "\n",
    "age_dict_children = dict(sorted(age_dict_children.items(), key=lambda item: item[1], reverse=True))\n",
    "df_age_children = pd.DataFrame.from_dict(age_dict_children, orient='index')\n",
    "#df_age_children.to_csv(r\"projects_csv/measures/agetotal_distribution.csv\")\n",
    "\n",
    "family_dict = dict(sorted(family_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "df_family = pd.DataFrame.from_dict(family_dict, orient='index')\n",
    "#df_family.to_csv(r\"projects_csv/measures/family_distribution.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns were then renaimed as \"*VARIABLE*\" and \"PERCENTAGE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filling missing values in CSVs using obtained distributions\n",
    "Now we need to use those values to fill missing cells in the remaining CSVs. WE will care about exceptions such as TupoPamoja and Mwalimu, that have restricted age range and consider the total age CSV just for MAJI, which is a project devoted to water supply: this could seem an uneveness in balance, since children are high in number but one of te interventions of MAJI regards only the school district, that by itself counts 900 inhabitants of younger ages with respect to other villagers. <br>\n",
    "In order to do so <a href=\"https://numpy.org/doc/stable/\">Numpy</a> was employed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_scan= [file for file in get_all_in_dir(\"projects_csv\")]\n",
    "\n",
    "df_age = pd.read_csv(\"projects_csv/measures/age_distribution.csv\")\n",
    "df_age_total = pd.read_csv(\"projects_csv/measures/agetotal_distribution.csv\")\n",
    "df_sex = pd.read_csv(\"projects_csv/measures/sex_distribution.csv\")\n",
    "df_family = pd.read_csv(\"projects_csv/measures/family_distribution.csv\")\n",
    "\n",
    "for file in to_scan:\n",
    "    df = pd.read_csv(file, encoding=\"utf-8\")\n",
    "    missing_age = df[\"AGE\"].isnull()\n",
    "    missing_sex = df[\"SEX\"].isnull()\n",
    "    missing_family = df[\"FAMILY\"].isnull()\n",
    "    if file.split(\"\\\\\")[1] not in [\"Maji.csv\", \"Mwalimu.csv\", \"TupoPamoja.csv\"]:\n",
    "        df.loc[missing_age, \"AGE\"] = np.random.choice(df_age[\"AGE\"], size=len(df[missing_age]), p=df_age[\"PERCENTAGE\"])\n",
    "    elif file.split(\"\\\\\")[1] not in [\"TupoPamoja.csv\", \"Mwalimu.csv\"]:\n",
    "        df.loc[missing_age, \"AGE\"] = np.random.choice(df_age_total[\"AGE\"], size=len(df[missing_age]), p=df_age_total[\"PERCENTAGE\"])\n",
    "    df.loc[missing_sex, \"SEX\"] = np.random.choice(df_sex[\"SEX\"], size=len(df[missing_sex]), p=df_sex[\"PERCENTAGE\"])\n",
    "    df.loc[missing_family, \"FAMILY\"] = np.random.choice(df_family[\"FAMILY\"], size=len(df[missing_family]), p=df_family[\"PERCENTAGE\"])\n",
    "    if file.split(\"\\\\\")[1] == \"TupoPamoja.csv\":\n",
    "        df.loc[missing_age, \"AGE\"] = np.random.choice(range(18, 49, 6), size=len(df[missing_age]))\n",
    "        for row in df.iterrows():\n",
    "            age = row[1][\"AGE\"]\n",
    "            idx = df[df['AGE']==age].index.values.astype(int)[0]\n",
    "            new_age = float(\"{:.2f}\".format(age/12))\n",
    "            df.loc[idx, \"AGE\"] = new_age\n",
    "    elif file.split(\"\\\\\")[1] == \"Mwalimu.csv\":\n",
    "        df.loc[missing_age, \"AGE\"] = np.random.choice(range(18, 25), size=len(df[missing_age]))\n",
    "   \n",
    "    #df.to_csv(r\"projects_csv/complete/complete_\"+file.split(\"\\\\\")[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the data\n",
    "After qualitative judgement of some data it was noticed that some names were reported differently from CSV to CSV and so a cleaning was required. <br>\n",
    "Projects that reported participants, outside villages described by the association were restricted to Mi.Fi.Ma. and the small amount of paticipants (less than 15) residing outside Pomerini were registered as Pomerinians."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_scan= [file for file in get_all_in_dir(\"projects_csv/complete\")]\n",
    "for file in to_scan:\n",
    "    df = pd.read_csv(file, encoding=\"utf-8\")\n",
    "    idx = df.index.values.astype(int)\n",
    "    for el in idx:\n",
    "        if df.loc[el, \"VILLAGE\"] == \"Kihesa Mgagao\":\n",
    "            df.loc[el, \"VILLAGE\"] = \"Khesa Mgagao\"\n",
    "        if df.loc[el, \"VILLAGE\"] in [\"Kaole\", \"Iringa\", \"Mbeya\", \"Lukani\", \"Ilula\", \"Ilula (Ikokoto)\", \"Image\", \"Dabaga\", \"Itungi\"]:\n",
    "            df.loc[el, \"VILLAGE\"] = \"Pomerini\"\n",
    "    #df.to_csv(r\"projects_csv/complete/\"+file.split(\"\\\\\")[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Villages complete files\n",
    "In order to represent the brader villages environment, random graphs were generated in Gephi: this was also done basing on the information given from the association and corresponds to their averaging of the population.<br>\n",
    "The villages under consideration were:\n",
    "<ol>\n",
    "<li><b>Khesa Mgagao</b>: ca. 2000 inhabitants </li>\n",
    "<li><b>Kitowo</b>: ca. 3000 inhabitants </li>\n",
    "<li><b>Mwawambala</b>: ca. 3000 inhabitants </li>\n",
    "<li><b>Pomerini</b>: ca. 3000 inhabitants </li>\n",
    "<li><b>Ukumbi</b>: ca. 3000 inhabitants </li>\n",
    "</ol>\n",
    "\n",
    "Basing on the information provided by the association, the oscillation in population and their unawareness of the real number of villagers, the number of inhabitants was set in a range of +- 250 around their averaging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_kitowo=random.randint(2750, 3250)\n",
    "rand_ukumbi=random.randint(2750, 3250)\n",
    "rand_mwawambala=random.randint(2750, 3250)\n",
    "rand_pomerini=random.randint(2750, 3250)\n",
    "rand_khesa_mgagao=random.randint(1750, 2250)\n",
    "\n",
    "id=0\n",
    "\n",
    "age_df=pd.read_csv(\"projects_csv/measures\\\\age_count.csv\", encoding=\"utf-8\")\n",
    "agetotal_df=pd.read_csv(\"projects_csv/measures\\\\agetotal_count.csv\", encoding=\"utf-8\")\n",
    "age_total_df = pd.concat([age_df,agetotal_df],ignore_index=True)\n",
    "idx = age_total_df.index.values.astype(int)\n",
    "total_age = age_total_df['PEOPLE'].sum() \n",
    "for el in idx:\n",
    "    age_total_df.loc[el, \"PEOPLE\"] = age_total_df.loc[el, \"PEOPLE\"]/total_age\n",
    "\n",
    "\n",
    "village_dict = {\"Kitowo\": rand_kitowo, \"Khesa Mgagao\": rand_khesa_mgagao, \"Mawambala\": rand_mwawambala, \"Pomerini\": rand_pomerini, \"Ukumbi\": rand_ukumbi}\n",
    "\n",
    "for village in village_dict.keys():\n",
    "    df=pd.DataFrame(index=np.arange(village_dict[village]), columns=[\"ID\",\"NAME\", \"PROJECT\", \"AGE\", \"SEX\", \"FAMILY\", \"START\", \"VILLAGE\"])\n",
    "    idx = df.index.values.astype(int)\n",
    "    missing_age = df[\"AGE\"].isnull()\n",
    "    missing_family=df[\"FAMILY\"].isnull()\n",
    "    missing_sex=df[\"SEX\"].isnull()\n",
    "    missing_id = df[\"ID\"].isnull()\n",
    "    df.loc[missing_age,'AGE'] = np.random.choice(age_total_df[\"AGE\"], size=len(df[missing_age]),p=age_total_df[\"PEOPLE\"] )\n",
    "    df.loc[missing_sex, \"SEX\"] = np.random.choice([\"M\", \"F\"], size=len(df[missing_sex]))\n",
    "    df.loc[missing_family, \"FAMILY\"] = np.random.choice(df_family[\"FAMILY\"], size=len(df[missing_family]), p=df_family[\"PERCENTAGE\"])\n",
    "    for el in idx[1:]:\n",
    "        df.loc[el, \"NAME\"] = \"Name\"+str(el)\n",
    "        df.loc[el, \"ID\"] = id+el\n",
    "        df.loc[el, \"VILLAGE\"] = village\n",
    "    df=df.dropna(thresh=4)\n",
    "    id=id+village_dict[village]-1\n",
    "    \n",
    "    #df.to_csv(r\"projects_csv/complete/complete_\"+village+\".csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the villages networks we can simply extract the data from the projects CSVs and substitute random generated rows with the ones belonging to projects set in that village. <br> \n",
    "In this way we will obtain a description of the situation for each village but, since what we want is a wider view on the project's influence on the community not only in villages but as a cross bridge between them, we ought to generate a complete network of Tulims's influence in the Pomerini zone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_scan= [file for file in get_all_in_dir(\"projects_csv/complete\")]\n",
    "villages = list()\n",
    "projects = list()\n",
    "for file in to_scan:\n",
    "    if file.split(\"/\")[1].split(\"\\\\\")[1].split(\"_\")[1].split(\".\")[0] in [\"Pomerini\", \"Mawambala\", \"Kitowo\", \"Khesa Mgagao\", \"Ukumbi\"]:\n",
    "        villages.append(file)\n",
    "    else:\n",
    "        projects.append(file)\n",
    "\n",
    "idx_dict = {\"Kitowo\": 1, \"Khesa Mgagao\": 1, \"Mawambala\": 1, \"Pomerini\": 1, \"Ukumbi\": 1}\n",
    "\n",
    "for project in projects:\n",
    "    df_project = pd.read_csv(project, encoding=\"utf-8\", index_col=\"Unnamed: 0\")\n",
    "    project_name = project.split(\"/\")[1].split(\"\\\\\")[1].split(\"_\")[1].split(\".\")[0]\n",
    "    idx = df_project.index.values.astype(int)\n",
    "    for el in idx:\n",
    "        village = df_project.loc[el, \"VILLAGE\"]\n",
    "        try:\n",
    "            df_village = pd.read_csv(\"projects_csv/complete/complete_\"+village+\".csv\", encoding=\"utf-8\", index_col=\"Unnamed: 0\")\n",
    "            df_village.loc[idx_dict[village], \"NAME\"] = df_project.loc[el, \"NAME\"]\n",
    "            df_village.loc[idx_dict[village], \"VILLAGE\"] = df_project.loc[el, \"VILLAGE\"]\n",
    "            df_village.loc[idx_dict[village], \"PROJECT\"] = project_name\n",
    "            df_village.loc[idx_dict[village], \"AGE\"] = df_project.loc[el, \"AGE\"]\n",
    "            df_village.loc[idx_dict[village], \"SEX\"] = df_project.loc[el, \"SEX\"]\n",
    "            df_village.loc[idx_dict[village], \"FAMILY\"] = df_project.loc[el, \"FAMILY\"]\n",
    "            df_village.loc[idx_dict[village], \"START\"] = df_project.loc[el, \"START\"]\n",
    "            #df_village.to_csv(r\"projects_csv/complete/complete_\"+village+\".csv\")\n",
    "            idx_dict[village]+=1\n",
    "        except Exception as e:\n",
    "            print (e)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can mesh up those networks and create a wider one containing all information regarding projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_scan= [file for file in get_all_in_dir(\"projects_csv/complete\")]\n",
    "villages = list()\n",
    "for file in to_scan:\n",
    "    if file.split(\"/\")[1].split(\"\\\\\")[1].split(\"_\")[1].split(\".\")[0] in [\"Pomerini\", \"Mawambala\", \"Kitowo\", \"Khesa Mgagao\", \"Ukumbi\"]:\n",
    "        villages.append(file)\n",
    "\n",
    "df_total=pd.DataFrame(columns=[\"ID\", \"NAME\", \"PROJECT\", \"AGE\", \"SEX\", \"FAMILY\", \"START\", \"VILLAGE\"])\n",
    "\n",
    "for village in villages:\n",
    "    village_name = village.split(\"/\")[1].split(\"\\\\\")[1].split(\"_\")[1].split(\".\")[0]\n",
    "    df = pd.read_csv(village, encoding=\"utf-8\", index_col=\"Unnamed: 0\")\n",
    "    df_total = pd.concat([df_total, df], ignore_index=True)\n",
    "    missing_village=df_total[\"VILLAGE\"].isnull()\n",
    "    df_total.loc[missing_village, \"VILLAGE\"] = village_name\n",
    "\n",
    "#df_total.to_csv(r\"projects_csv/complete/whole_network.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can manually migrate files in a dedicated folder, namely \"villages\" and set the id as index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating village networks\n",
    "Now, in order to create our network we need to set rules in order to discretely decide if two nodes are connected. <br>\n",
    "The main discriminants would be: \n",
    "<ol>\n",
    "<li><b>Being part of the same project</b>: this should be the most valuable kind of connection, of weight 3, since what we are trying to investigate are the connections created by the projects; </li>\n",
    "<li><b>Being part of the same family</b>: this kind of connection has definitely a heavier weight, since people living in the same family core should exchange information and positive or negative reinforcements that can weight on project's efficience; </li>\n",
    "<li><b>Being part of the same village</b></li>\n",
    "</ol>\n",
    "\n",
    "So now every node should present:\n",
    " <ol>\n",
    "<li>A number of connection of <b>weight 2</b> equal to the <b>number of participants in its project minus one</b> (itself)</li>\n",
    "<li>A number of connection of <b>weight 1</b> equal to the <b>number of people in its family core</b></li>\n",
    "</ol>\n",
    "\n",
    "Also in this phase it was possible to balance incoherences in family components distribution: simply after a first cycle of edges is created it is possible that some names are still present in the starting dataset.<br>\n",
    "This is because it is very difficult for families to be distributed so that at the end every component is in a core: the problem was solved by grouping leftovers as if they were a family core on their own of size equal to their quantity. Anyway it is impossible that this quantity exceeds the maximum amount of people per family, because otherway some of them would group together and lower again the quantity below 8, which is the maximum width of family cores.<br>\n",
    "This new family core is not only included as a set of edges but substituted to the previous in the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Unnamed: 0'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\GIORGI~1\\AppData\\Local\\Temp/ipykernel_9244/3859560.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    119\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m                 \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m         \u001b[0mdf_network\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_network\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Unnamed: 0\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m         \u001b[0mdf_network\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_network\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ID\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[0mdf_network\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"projects_csv/networks/\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"_edges.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4904\u001b[0m                 \u001b[0mweight\u001b[0m  \u001b[1;36m1.0\u001b[0m     \u001b[1;36m0.8\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4905\u001b[0m         \"\"\"\n\u001b[1;32m-> 4906\u001b[1;33m         return super().drop(\n\u001b[0m\u001b[0;32m   4907\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4908\u001b[0m             \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4148\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4149\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4150\u001b[1;33m                 \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4152\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[1;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[0;32m   4183\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4184\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4185\u001b[1;33m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4186\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   6015\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6016\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6017\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{labels[mask]} not found in axis\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6018\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6019\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['Unnamed: 0'] not found in axis\""
     ]
    }
   ],
   "source": [
    "to_scan= [file for file in get_all_in_dir(\"projects_csv/villages\")]\n",
    "\n",
    "for file in to_scan:\n",
    "    df = pd.read_csv(file, encoding=\"utf-8\", index_col=\"Unnamed: 0\")\n",
    "    df = df.sample(frac=1, ignore_index=True)\n",
    "\n",
    "    df_network = pd.DataFrame(columns=[\"SOURCE\", \"TARGET\", \"WEIGHT\"])\n",
    "    df_node = pd.DataFrame(columns=[\"SOURCE\", \"TARGET\", \"WEIGHT\"])\n",
    "\n",
    "    if file != \"projects_csv/villages\\\\whole_network.csv\":\n",
    "        file_name = file.split(\"/\")[1].split(\"\\\\\")[1].split(\"_\")[1].split(\".\")[0]\n",
    "        idx = df.index.values.astype(int)\n",
    "        name_list = list()\n",
    "        for el in idx:\n",
    "            try:\n",
    "                n=1\n",
    "                for el_2 in idx:\n",
    "                    if n<=int(df.loc[el, \"FAMILY\"]):\n",
    "                        if df.loc[el, \"FAMILY\"] == df.loc[el_2, \"FAMILY\"] and df.loc[el, \"ID\"] != df.loc[el_2, \"ID\"]:\n",
    "                            df_node.loc[1,\"SOURCE\"] = df.loc[el, \"ID\"]\n",
    "                            df_node.loc[1,\"TARGET\"] = df.loc[el_2, \"ID\"]\n",
    "                            df_node.loc[1,\"WEIGHT\"] = 1\n",
    "                            df_network = pd.concat([df_network, df_node],ignore_index=True)\n",
    "                            family_count = int(df.loc[el, \"FAMILY\"])\n",
    "                            n+=1\n",
    "                            df = df.drop(el_2)\n",
    "                            idx = df.index.values.astype(int)\n",
    "                        else:\n",
    "                            continue     \n",
    "                    else:\n",
    "                        idx_last_edge = len(df_network)-1\n",
    "                        for i in range(family_count):\n",
    "                            target_count=family_count-(i+1)\n",
    "                            c=1\n",
    "                            for z in range(target_count):\n",
    "                                pointer_source = idx_last_edge-i\n",
    "                                pointer = pointer_source-c\n",
    "                                df_node.loc[1,\"SOURCE\"] = df_network.loc[pointer_source, \"TARGET\"]\n",
    "                                df_node.loc[1,\"TARGET\"] = df_network.loc[pointer, \"TARGET\"]\n",
    "                                df_node.loc[1,\"WEIGHT\"] = 1\n",
    "                                df_network = pd.concat([df_network, df_node], ignore_index=True)\n",
    "                                c+=1\n",
    "                        df = df.drop(el)\n",
    "                        idx = df.index.values.astype(int)\n",
    "                        break\n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "\n",
    "        if len(df)>0:\n",
    "            idx = df.index.values.astype(int)\n",
    "            for el in idx[:1]:\n",
    "                name_list.append(df.loc[el, \"ID\"])\n",
    "                for el_2 in idx[1:]:\n",
    "                    df_node.loc[1,\"SOURCE\"] = df.loc[el, \"ID\"]\n",
    "                    df_node.loc[1,\"TARGET\"] = df.loc[el_2, \"ID\"]\n",
    "                    df_node.loc[1,\"WEIGHT\"] = 1\n",
    "                    df_network = pd.concat([df_network, df_node], ignore_index = True)\n",
    "                    family_count = int(df.loc[el, \"FAMILY\"])\n",
    "                    name_list.append(df.loc[el_2, \"ID\"])\n",
    "                    df = df.drop(el_2)\n",
    "            idx_last_edge = len(df_network)-1\n",
    "            for i in range(family_count):\n",
    "                target_count=family_count-(i+1)\n",
    "                c=1\n",
    "                for z in range(target_count):\n",
    "                    pointer_source = idx_last_edge-i\n",
    "                    pointer = pointer_source-c\n",
    "                    df_node.loc[1,\"SOURCE\"] = df_network.loc[pointer_source, \"TARGET\"]\n",
    "                    df_node.loc[1,\"TARGET\"] = df_network.loc[pointer, \"TARGET\"]\n",
    "                    df_node.loc[1,\"WEIGHT\"] = 1\n",
    "                    df_network = pd.concat([df_network, df_node], ignore_index=True)\n",
    "                    c+=1\n",
    "            df = df.drop(el)\n",
    "        \n",
    "\n",
    "        df = pd.read_csv(file, encoding=\"utf-8\", index_col=\"Unnamed: 0\")\n",
    "        df_whole = pd.read_csv(\"projects_csv/villages\\\\whole_network.csv\", encoding=\"utf-8\", index_col=\"Unnamed: 0\")\n",
    "\n",
    "        idx = df.index.values.astype(int)\n",
    "        for el in idx:\n",
    "            if df.loc[el, \"ID\"] in name_list:\n",
    "                name = df.loc[el, \"ID\"]\n",
    "                df.loc[el, \"FAMILY\"] = len(name_list)-1\n",
    "                idx_name = df_whole.index[(df_whole[\"ID\"] == name) & (df_whole[\"VILLAGE\"] == file_name)]\n",
    "                df_whole.loc[idx_name, \"FAMILY\"] = len(name_list)-1\n",
    "        \n",
    "        df.to_csv(r\"projects_csv/villages/complete_\"+file_name+\".csv\")\n",
    "        df_whole.to_csv(r\"projects_csv/villages\\\\whole_network.csv\")\n",
    "\n",
    "        idx = df.index.values.astype(int)\n",
    "        for el in idx:\n",
    "            try:\n",
    "                project_count = 0\n",
    "                if df.loc[el, \"PROJECT\"] != \"Maji\":\n",
    "                    for el_2 in idx[el:]:\n",
    "                        if df.loc[el, \"PROJECT\"] == df.loc[el_2, \"PROJECT\"] and df.loc[el, \"ID\"] != df.loc[el_2, \"ID\"]:\n",
    "                            df_node.loc[1,\"SOURCE\"] = df.loc[el, \"ID\"]\n",
    "                            df_node.loc[1,\"TARGET\"] = df.loc[el_2, \"ID\"]\n",
    "                            df_node.loc[1,\"WEIGHT\"] = 2\n",
    "                            df_network = pd.concat([df_network, df_node],ignore_index=True)\n",
    "                            project_count +=1\n",
    "                            df = df.drop(el_2) \n",
    "                        else:\n",
    "                            continue        \n",
    "                    idx_last_edge = len(df_network)-1\n",
    "                    for i in range(project_count):\n",
    "                        target_count=project_count-(i+1)\n",
    "                        c=1\n",
    "                        for z in range(target_count):\n",
    "                            pointer_source = idx_last_edge-i\n",
    "                            pointer = pointer_source-c\n",
    "                            df_node.loc[1,\"SOURCE\"] = df_network.loc[pointer_source, \"TARGET\"]\n",
    "                            df_node.loc[1,\"TARGET\"] = df_network.loc[pointer, \"TARGET\"]\n",
    "                            df_node.loc[1,\"WEIGHT\"] = 2\n",
    "                            df_network = pd.concat([df_network, df_node], ignore_index=True)\n",
    "                            c+=1\n",
    "                    df = df.drop(el)\n",
    "            except Exception as e:\n",
    "                continue        \n",
    "            \n",
    "        df_network = df_network.set_index(\"ID\")\n",
    "        df_network.to_csv(r\"projects_csv/networks/\"+file_name+\"_edges.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a file containing edges of the whole network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_scan= [file for file in get_all_in_dir(\"projects_csv/networks\")]\n",
    "df_whole_edges = pd.DataFrame(columns=[\"SOURCE\", \"TARGET\", \"WEIGHT\"])\n",
    "for file in to_scan:\n",
    "    df = pd.read_csv(file, encoding=\"utf-8\")\n",
    "    df_whole_edges = pd.concat([df_whole_edges, df], ignore_index=True)\n",
    "\n",
    "df=pd.read_csv(\"projects_csv/villages\\\\whole_network.csv\", encoding=\"utf-8\", index_col=\"Unnamed: 0\")\n",
    "idx = df.index.values.astype(int)\n",
    "for el in idx:\n",
    "    try:\n",
    "        project_count = 0\n",
    "        if df.loc[el, \"PROJECT\"] != \"Maji\":\n",
    "            for el_2 in idx[el:]:\n",
    "                if df.loc[el, \"PROJECT\"] == df.loc[el_2, \"PROJECT\"] and df.loc[el, \"ID\"] != df.loc[el_2, \"ID\"] and df.loc[el, \"VILLAGE\"] != df.loc[el_2, \"VILLAGE\"]:\n",
    "                    df_node.loc[1,\"SOURCE\"] = df.loc[el, \"ID\"]\n",
    "                    df_node.loc[1,\"TARGET\"] = df.loc[el_2, \"ID\"]\n",
    "                    df_node.loc[1,\"WEIGHT\"] = 2\n",
    "                    df_whole_edges = pd.concat([df_whole_edges, df_node],ignore_index=True)\n",
    "                    project_count +=1\n",
    "                    df = df.drop(el_2) \n",
    "                else:\n",
    "                    continue        \n",
    "            idx_last_edge = len(df_whole_edges)-1\n",
    "            for i in range(project_count):\n",
    "                target_count=project_count-(i+1)\n",
    "                c=1\n",
    "                for z in range(target_count):\n",
    "                    pointer_source = idx_last_edge-i\n",
    "                    pointer = pointer_source-c\n",
    "                    df_node.loc[1,\"SOURCE\"] = df_whole_edges.loc[pointer_source, \"TARGET\"]\n",
    "                    df_node.loc[1,\"TARGET\"] = df_whole_edges.loc[pointer, \"TARGET\"]\n",
    "                    df_node.loc[1,\"WEIGHT\"] = 2\n",
    "                    df_whole_edges = pd.concat([df_whole_edges, df_node], ignore_index=True)\n",
    "                    c+=1\n",
    "            df = df.drop(el)\n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "df_whole_edges = df_whole_edges.drop_duplicates(keep='last')\n",
    "df_whole_edges = df_whole_edges.drop(\"Unnamed: 0\", axis=1)\n",
    "df_whole_edges = df_whole_edges.set_index(\"ID\")\n",
    "#df_whole_edges.to_csv(r\"projects_csv/networks/whole_edges.csv\")              \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last thing left to do is clean the data as to process it in Gephi, by removing empty cells. <br>\n",
    "We might want to remove isolated nodes in order to obtain a network representing only interlaced communities and return this value to have an idea of how many people are left without conections in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_scan= [file for file in get_all_in_dir(\"projects_csv/villages\")]\n",
    "for file in to_scan:\n",
    "    file_name = file.split(\"/\")[1].split(\"\\\\\")[1].split(\"_\")[1].split(\".\")[0]\n",
    "    df = pd.read_csv(file, encoding=\"utf-8\")\n",
    "    df = df[df[\"FAMILY\"] != 0]\n",
    "    df = df.replace(r'^\\s*$', np.nan, regex=True)\n",
    "    df= df.drop(\"Unnamed: 0\", axis=1)\n",
    "    df[[\"AGE\", \"FAMILY\", \"START\"]] = df[[\"AGE\", \"FAMILY\", \"START\"]].apply(pd.to_numeric)\n",
    "    #df.to_csv(r\"projects_csv/networks/\"+file_name+\"_nodes.csv\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  SOURCE  TARGET  WEIGHT\n",
      "0           0    3962    3415       1\n",
      "1           1    3962    3965       1\n",
      "2           2    3962    4869       1\n",
      "3           3    3962    4724       1\n",
      "4           4    4724    4869       1\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of ['ID'] are in the columns\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\GIORGI~1\\AppData\\Local\\Temp/ipykernel_15420/972939844.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"edges\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ID\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[1;31m#df.to_csv(r\"projects_csv/networks/\"+file_name+\"_nodes.csv\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mset_index\u001b[1;34m(self, keys, drop, append, inplace, verify_integrity)\u001b[0m\n\u001b[0;32m   5449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5450\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmissing\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5451\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"None of {missing} are in the columns\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5452\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5453\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of ['ID'] are in the columns\""
     ]
    }
   ],
   "source": [
    "to_scan= [file for file in get_all_in_dir(\"projects_csv/networks\")]\n",
    "for file in to_scan:\n",
    "    file_name = file.split(\"/\")[1].split(\"\\\\\")[1].split(\"_\")[1].split(\".\")[0]\n",
    "    df = pd.read_csv(file, encoding=\"utf-8\")\n",
    "    print(df.head())\n",
    "    if file_name == \"edges\": \n",
    "        df = df.set_index(\"ID\")\n",
    "    #df.to_csv(r\"projects_csv/networks/\"+file_name+\"_nodes.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add data regarding friends and working relatioship\n",
    " \n",
    "Another possible approach is to use literature on this topic to extract data regarding significant communities such as friendship and co-working. <br>\n",
    "\n",
    "In \"The role of strong-tie social networks in mediating food security of fish\n",
    "resources by a traditional riverine community in the Brazilian Amazon\" (2015, F. Merthes et al.) communities based on friendship have been find to fall in a range from 2 to 29 nodes connected; also work communities fall in a similar range, from 2 to 30. In the study it was found that, upon 793 relationships, 177 were of friendship kind, 167 of working type and 571 based on kinship.<br>\n",
    "\n",
    "From this data, and the distribution of kinship relationship we have at disposal, we can easily recreate a model that follows the distribution evidentiated in this study and fills the gaps created by missing edge weights. This model should present nodes with edges strctured as follows:\n",
    " <ol>\n",
    "<li>A number of connection of <b>weight 2</b> equal to the <b>number of participants in its project minus one</b> (itself)</li>\n",
    "<li>A number of connection of <b>weight 1</b> equal to the <b>number of people in its family core</b> plus the <b>number of people in its friends and co-working community minus one</b> (itself)</li>\n",
    "</ol> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_scan= [file for file in get_all_in_dir(\"projects_csv/networks\")]\n",
    "for file in to_scan:\n",
    "    df = pd.read_csv(file, encoding=\"utf-8\", index_col=\"Unnamed: 0\")\n",
    "    conversion_factor = df[df.columns[\"WEIGHT\"]==1].count()/571\n",
    "    friendship_ties = 177*conversion_factor\n",
    "    working_ties = 167*conversion_factor\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1d603ae3e84bc2c7668b84db465d2afb033d38d826ce98f701f29ef4e883b1f2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
